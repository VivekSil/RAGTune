{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import AdditionalConfig, Timeout\n",
    "\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# For connecting to Weaviate cloud\n",
    "# client = weaviate.connect_to_weaviate_cloud(cluster_url=os.environ[\"WEAVIATE_DB_URL\"], auth_credentials=weaviate.auth.AuthApiKey(os.environ[\"WEAVIATE_API_KEY\"]))\n",
    "\n",
    "# For connecting to local weaviate instance\n",
    "# client = weaviate.connect_to_local(\n",
    "#     additional_config=AdditionalConfig(timeout=Timeout(init=1000, query=1090, insert=1580)),\n",
    "# )\n",
    "\n",
    "client = weaviate.connect_to_embedded(\n",
    "    environment_variables={\"ENABLE_MODULES\": \"text2vec-transformers,generative-ollama\", \"TRANSFORMERS_INFERENCE_API\":\"http://localhost:8080\"},\n",
    "    additional_config=AdditionalConfig(timeout=Timeout(init=10000, query=10900, insert=15800)),\n",
    "    version=\"1.25.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import classes as wvc\n",
    "client.collections.delete(\"OllamaCollection\")\n",
    "# lets create the collection, specifing our base url accordingling\n",
    "collection = client.collections.create(\n",
    "    \"Research_papers\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_transformers(),\n",
    "    generative_config=wvc.config.Configure.Generative.ollama(\n",
    "        api_endpoint=\"http://localhost:11434\",\n",
    "        model=\"phi3\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_TRANSFORMERS: 'text2vec-transformers'>, model={'poolingStrategy': 'masked_mean'}, vectorize_collection_name=True)\n",
      "_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'apiEndpoint': 'http://localhost:11434', 'model': 'phi3'})\n"
     ]
    }
   ],
   "source": [
    "# Let's check our collection\n",
    "print(collection.config.get().vectorizer_config)\n",
    "print(collection.config.get().generative_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pypdf\n",
    "\n",
    "def merge_lines(text_lines):\n",
    "    chunks = []\n",
    "    while text_lines:\n",
    "        chunks.append('\\n'.join(text_lines[:10]))\n",
    "        if len(text_lines) <= 9:\n",
    "            return chunks\n",
    "        text_lines = text_lines[10:]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "documents = {}\n",
    "# files_list = os.listdir(\"./data\")\n",
    "files_list = ['./ragtune-backend/2405.15731v1.pdf']\n",
    "for file_name in files_list:\n",
    "    documents[file_name] = []\n",
    "    reader = PdfReader(f\"{file_name}\")\n",
    "    # reader = PdfReader(f\"./data/{file_name}\")\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text().split(\"\\n\")\n",
    "        chunk = merge_lines(text)\n",
    "        documents[file_name].extend(chunk)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with collection.batch.dynamic() as batch:\n",
    "  for doc in documents:\n",
    "    for chunk in documents[doc]:\n",
    "      batch.add_object(\n",
    "          properties = {\"content\" : chunk, \"source\":doc},\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "from dspy import OllamaLocal\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "lm = dspy.GROQ(model='llama3-8b-8192', api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "# Can be used locally, but slower\n",
    "# lm = dspy.OllamaLocal(model='phi3')\n",
    "retriever = WeaviateRM(\"Research_papers\", weaviate_client=client)\n",
    "\n",
    "dspy.settings.configure(rm=retriever, lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleSignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question and context. You need to answer the question with explanation based on the context given. If the answer doesn't lie in the context, say I don't know\"\"\"\n",
    "    question = dspy.InputField(desc=\"Question asked\")\n",
    "    context = dspy.InputField(desc=\"Potentially related passages\")\n",
    "    answer = dspy.OutputField(desc=\"Answer to the question based on the given context, just give answer, and nothing else\")\n",
    "\n",
    "class DistractorSignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question and context. You need to answer the question with explanation based on the context given. If the answer doesn't lie in the context, say I don't know\"\"\"\n",
    "    question = dspy.InputField(desc=\"Question asked\")\n",
    "    context = dspy.InputField(desc=\"Potentially unrelated passages\")\n",
    "    answer = dspy.OutputField(desc=\"Give 2-3 lines reason you cannot answer the given question as the context is insufficient or unrelated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleRAFT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.retrieve = dspy.Retrieve(k=5)\n",
    "        self.generate_answer = dspy.ChainOfThought(OracleSignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).toDict()[\"passages\"]\n",
    "        prediction = self.generate_answer(question=question, context=context)\n",
    "        # return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistractorRAFT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.retrieve = dspy.Retrieve(k=30)\n",
    "        self.generate_answer = dspy.ChainOfThought(DistractorSignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).toDict()[\"passages\"][-5:]\n",
    "        prediction = self.generate_answer(question=question ,context=context)\n",
    "        # return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q textdistance evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import cosine\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_oracle(example, pred):\n",
    "    return cosine(example, pred)\n",
    "\n",
    "def cosine_sim_distractor(example, pred):\n",
    "    return 1-cosine(example, pred)\n",
    "\n",
    "def embedding_sim_oracle(question, pred):\n",
    "    question_embedding = model.encode(question.question)\n",
    "    pred_embedding = model.encode(pred.rationale.split(\"Answer:\")[-1])\n",
    "    return float(util.dot_score(question_embedding, pred_embedding)[0][0])\n",
    "\n",
    "def embedding_sim_distractor(question, pred):\n",
    "    example_embedding = model.encode(\"I don't know the answer to the question\")\n",
    "    pred_embedding = model.encode(pred.answer.split(\"Answer:\")[-1])\n",
    "    if pred.answer.split(\"Answer:\")[-1] == \"I don't know\":\n",
    "        return float(util.dot_score(example_embedding, pred_embedding)[0][0]) - 0.2\n",
    "    return float(util.dot_score(example_embedding, pred_embedding)[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import COPRO\n",
    "\n",
    "teleprompter_oracle = COPRO(metric=embedding_sim_oracle, depth=1, breadth=2)\n",
    "teleprompter_distractor = COPRO(=embedding_sim_distractor, depth=1, breadth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Describe recurrent neural networks ?\", \"What is the subspaced similarity between different r in LoRA ?\", \"Describe scaled dot product attention\"]\n",
    "answers = [\n",
    "    \"Recurrent Neural Networks (RNNs) are specialized neural networks designed for sequential data processing. They maintain a hidden state that captures information from past inputs, allowing them to exhibit dynamic temporal behavior. RNNs employ parameter sharing across time steps, enabling them to process sequences of varying lengths efficiently. Common applications include natural language processing, time series analysis, and speech recognition. Despite their effectiveness, RNNs struggle with capturing long-term dependencies due to vanishing or exploding gradients and are inefficient in parallel processing. Architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have been developed to address these limitations while retaining the core recurrent structure. RNNs remain fundamental in sequence modeling tasks but are often supplemented or replaced by more advanced architectures in scenarios requiring long-range dependencies and improved performance.\",\n",
    "\n",
    "    \"In LoRA (Linearly-organized Recurrent Attention), the `subspaced similarity between different r` refers to a measure of similarity between different attention contexts (represented by `r`) within the model. LoRA introduces the concept of `subspaces` to the attention mechanism. Each attention context `r` is projected onto multiple subspaces, and the similarity between different attention contexts is measured within these subspaces. The subspaced similarity is computed using a dot product between the projected representations of attention contexts onto each subspace. By calculating similarity in multiple subspaces, LoRA allows for capturing diverse types of relationships between attention contexts, thereby enhancing the model's ability to capture nuanced dependencies in sequential data. This approach helps LoRA effectively model complex sequences by providing more flexibility and expressiveness in the attention mechanism.\",\n",
    "\n",
    "    \"Scaled dot-product attention is a pivotal component of transformer models, facilitating effective capture of dependencies across input sequences. It operates by computing the dot product of query and key vectors, scaled to prevent vanishing gradients, followed by a softmax to obtain attention weights. These weights indicate the relevance of each value vector to its corresponding query. Ultimately, a weighted sum of the value vectors, weighted by the attention scores, produces the output. This attention mechanism enables the model to focus on pertinent information while processing input sequences, facilitating tasks such as machine translation, text generation, and language understanding. Its ability to capture long-range dependencies efficiently has contributed to the success of transformer-based architectures in various natural language processing applications.\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [dspy.Example(question=questions[i], answer=answers[i]).with_inputs(\"question\") for i in range(len(questions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]{'content': ['transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental', 'constraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].', '3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n2'], 'source': '1706.03762v5.pdf'}\n",
      "Average Metric: 0.6492495536804199 / 1  (64.9):  33%|███▎      | 1/3 [00:05<00:10,  5.43s/it]{'content': ['Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1≤i≤8) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1≤j≤64)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\nφ(Ar=8,Ar=64,i,j) =||Ui⊤\\nAr=8Uj\\nAr=64||2', 'F\\nmin(i,j)∈[0,1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\nφ(·)has a range of [0,1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how φchanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1', '6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58', 'j12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46', '52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both ∆Wqand∆Wv.\\nThe third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top', 'directions in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiﬁcantly between\\nAr=8andAr=64, while others do not. Speciﬁcally, ∆Wv(resp. ∆Wq) ofAr=8\\nand∆Wv(resp. ∆Wq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0.5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix', 'can indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conﬁrm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n∆Wqappears to have a higher “intrinsic rank” than ∆Wv, since more common singular value direc-\\ntions are learned by both runs for ∆Wq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX ∆WCOMPARE TO W?\\nWe further investigate the relationship between ∆WandW. In particular, does ∆Whighly correlate\\nwithW? (Or mathematically, is ∆Wmostly contained in the top singular directions of W?) Also,', '7Note that a similar analysis can be carried out with Band the left-singular unitary matrices – we stick with\\nAfor our experiments.\\n11'], 'source': '2106.09685v2.pdf'}\n",
      "Average Metric: 0.695642776787281 / 2  (34.8):  67%|██████▋   | 2/3 [00:11<00:05,  5.70s/it] {'content': ['Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together', 'into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.', 'While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as', 'depicted in Figure 2.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=∑dk\\ni=1qiki, has mean 0and variance dk.\\n4'], 'source': '1706.03762v5.pdf'}\n",
      "Average Metric: 1.438035674393177 / 3  (47.9): 100%|██████████| 3/3 [00:15<00:00,  5.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_74e42 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_74e42 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_74e42_row0_col0, #T_74e42_row0_col1, #T_74e42_row0_col2, #T_74e42_row0_col3, #T_74e42_row0_col4, #T_74e42_row1_col0, #T_74e42_row1_col1, #T_74e42_row1_col2, #T_74e42_row1_col3, #T_74e42_row1_col4, #T_74e42_row2_col0, #T_74e42_row2_col1, #T_74e42_row2_col2, #T_74e42_row2_col3, #T_74e42_row2_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_74e42\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_74e42_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_74e42_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_74e42_level0_col2\" class=\"col_heading level0 col2\" >rationale</th>\n",
       "      <th id=\"T_74e42_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_74e42_level0_col4\" class=\"col_heading level0 col4\" >embedding_sim_oracle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_74e42_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_74e42_row0_col0\" class=\"data row0 col0\" >Describe recurrent neural networks ?</td>\n",
       "      <td id=\"T_74e42_row0_col1\" class=\"data row0 col1\" >Recurrent Neural Networks (RNNs) are specialized neural networks designed for sequential data processing. They maintain a hidden state that captures information from past inputs, allowing...</td>\n",
       "      <td id=\"T_74e42_row0_col2\" class=\"data row0 col2\" >Describe recurrent neural networks? Answer: Recurrent neural networks (RNNs) typically factor computation along the symbol positions of the input and output sequences. They generate a...</td>\n",
       "      <td id=\"T_74e42_row0_col3\" class=\"data row0 col3\" >Here is the answer: Describe recurrent neural networks? Here's an answer based on the provided context: Recurrent neural networks (RNNs) typically factor computation along the...</td>\n",
       "      <td id=\"T_74e42_row0_col4\" class=\"data row0 col4\" >✔️ [0.6492495536804199]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_74e42_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_74e42_row1_col0\" class=\"data row1 col0\" >What is the subspaced similarity between different r in LoRA ?</td>\n",
       "      <td id=\"T_74e42_row1_col1\" class=\"data row1 col1\" >In LoRA (Linearly-organized Recurrent Attention), the `subspaced similarity between different r` refers to a measure of similarity between different attention contexts (represented by `r`) within...</td>\n",
       "      <td id=\"T_74e42_row1_col2\" class=\"data row1 col2\" >Please provide the question and context, and I'll be happy to help you answer it based on the given context.</td>\n",
       "      <td id=\"T_74e42_row1_col3\" class=\"data row1 col3\" >I'm excited to start!\n",
       "\n",
       "Please provide the question and context, and I'll be happy to help you answer it based on the given context.</td>\n",
       "      <td id=\"T_74e42_row1_col4\" class=\"data row1 col4\" >✔️ [0.046393223106861115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_74e42_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_74e42_row2_col0\" class=\"data row2 col0\" >Describe scaled dot product attention</td>\n",
       "      <td id=\"T_74e42_row2_col1\" class=\"data row2 col1\" >Scaled dot-product attention is a pivotal component of transformer models, facilitating effective capture of dependencies across input sequences. It operates by computing the dot product...</td>\n",
       "      <td id=\"T_74e42_row2_col2\" class=\"data row2 col2\" >Describe scaled dot-product attention. Answer: The scaled dot-product attention is a type of attention mechanism used in the Transformer model. It is called \"scaled\" because...</td>\n",
       "      <td id=\"T_74e42_row2_col3\" class=\"data row2 col3\" >Question: Describe scaled dot-product attention Context: [1] «['Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running...</td>\n",
       "      <td id=\"T_74e42_row2_col4\" class=\"data row2 col4\" >✔️ [0.742392897605896]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4ff4a85cf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]{'content': ['transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental', 'constraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].', '3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n2'], 'source': '1706.03762v5.pdf'}\n",
      "Average Metric: 0.7815989255905151 / 1  (78.2):  33%|███▎      | 1/3 [00:02<00:04,  2.05s/it]{'content': ['Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1≤i≤8) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1≤j≤64)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\nφ(Ar=8,Ar=64,i,j) =||Ui⊤\\nAr=8Uj\\nAr=64||2', 'F\\nmin(i,j)∈[0,1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\nφ(·)has a range of [0,1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how φchanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1', '6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58', 'j12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46', '52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both ∆Wqand∆Wv.\\nThe third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top', 'directions in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiﬁcantly between\\nAr=8andAr=64, while others do not. Speciﬁcally, ∆Wv(resp. ∆Wq) ofAr=8\\nand∆Wv(resp. ∆Wq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0.5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix', 'can indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conﬁrm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n∆Wqappears to have a higher “intrinsic rank” than ∆Wv, since more common singular value direc-\\ntions are learned by both runs for ∆Wq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX ∆WCOMPARE TO W?\\nWe further investigate the relationship between ∆WandW. In particular, does ∆Whighly correlate\\nwithW? (Or mathematically, is ∆Wmostly contained in the top singular directions of W?) Also,', '7Note that a similar analysis can be carried out with Band the left-singular unitary matrices – we stick with\\nAfor our experiments.\\n11'], 'source': '2106.09685v2.pdf'}\n",
      "Average Metric: 0.8398906514048576 / 2  (42.0):  67%|██████▋   | 2/3 [00:06<00:03,  3.54s/it]{'content': ['Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together', 'into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.', 'While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as', 'depicted in Figure 2.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=∑dk\\ni=1qiki, has mean 0and variance dk.\\n4'], 'source': '1706.03762v5.pdf'}\n",
      "Average Metric: 1.5419646427035332 / 3  (51.4): 100%|██████████| 3/3 [00:10<00:00,  3.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_059e9 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_059e9 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_059e9_row0_col0, #T_059e9_row0_col1, #T_059e9_row0_col2, #T_059e9_row0_col3, #T_059e9_row0_col4, #T_059e9_row1_col0, #T_059e9_row1_col1, #T_059e9_row1_col2, #T_059e9_row1_col3, #T_059e9_row1_col4, #T_059e9_row2_col0, #T_059e9_row2_col1, #T_059e9_row2_col2, #T_059e9_row2_col3, #T_059e9_row2_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_059e9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_059e9_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_059e9_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_059e9_level0_col2\" class=\"col_heading level0 col2\" >rationale</th>\n",
       "      <th id=\"T_059e9_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_059e9_level0_col4\" class=\"col_heading level0 col4\" >embedding_sim_oracle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_059e9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_059e9_row0_col0\" class=\"data row0 col0\" >Describe recurrent neural networks ?</td>\n",
       "      <td id=\"T_059e9_row0_col1\" class=\"data row0 col1\" >Recurrent Neural Networks (RNNs) are specialized neural networks designed for sequential data processing. They maintain a hidden state that captures information from past inputs, allowing...</td>\n",
       "      <td id=\"T_059e9_row0_col2\" class=\"data row0 col2\" >Question: Describe recurrent neural networks ?\n",
       "\n",
       "Context: Potentially related passages\n",
       "\n",
       "Reasoning: Let's think step by step in order to</td>\n",
       "      <td id=\"T_059e9_row0_col3\" class=\"data row0 col3\" >The context does not explicitly describe recurrent neural networks. However, it mentions recurrent neural networks (RNNs) in the context of sequence modeling and transduction models.</td>\n",
       "      <td id=\"T_059e9_row0_col4\" class=\"data row0 col4\" >✔️ [0.7815989255905151]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_059e9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_059e9_row1_col0\" class=\"data row1 col0\" >What is the subspaced similarity between different r in LoRA ?</td>\n",
       "      <td id=\"T_059e9_row1_col1\" class=\"data row1 col1\" >In LoRA (Linearly-organized Recurrent Attention), the `subspaced similarity between different r` refers to a measure of similarity between different attention contexts (represented by `r`) within...</td>\n",
       "      <td id=\"T_059e9_row1_col2\" class=\"data row1 col2\" >I apologize, but it seems that the context provided is a collection of passages from a research paper, and the question is not explicitly stated....</td>\n",
       "      <td id=\"T_059e9_row1_col3\" class=\"data row1 col3\" >I don't know</td>\n",
       "      <td id=\"T_059e9_row1_col4\" class=\"data row1 col4\" >✔️ [0.0582917258143425]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_059e9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_059e9_row2_col0\" class=\"data row2 col0\" >Describe scaled dot product attention</td>\n",
       "      <td id=\"T_059e9_row2_col1\" class=\"data row2 col1\" >Scaled dot-product attention is a pivotal component of transformer models, facilitating effective capture of dependencies across input sequences. It operates by computing the dot product...</td>\n",
       "      <td id=\"T_059e9_row2_col2\" class=\"data row2 col2\" >Question: Describe scaled dot-product attention Context: [1] «['Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running...</td>\n",
       "      <td id=\"T_059e9_row2_col3\" class=\"data row2 col3\" >Scaled Dot-Product Attention</td>\n",
       "      <td id=\"T_059e9_row2_col4\" class=\"data row2 col4\" >✔️ [0.7020739912986755]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4ff4897880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = dict(num_threads=1, display_progress=True, display_table=3)\n",
    "compiled_oracle = teleprompter_oracle.compile(OracleRAFT(),trainset=trainset, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.143144190311432 / 3  (71.4): 100%|██████████| 3/3 [00:10<00:00,  3.54s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_70c3f th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_70c3f td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_70c3f_row0_col0, #T_70c3f_row0_col1, #T_70c3f_row0_col2, #T_70c3f_row0_col3, #T_70c3f_row0_col4, #T_70c3f_row1_col0, #T_70c3f_row1_col1, #T_70c3f_row1_col2, #T_70c3f_row1_col3, #T_70c3f_row1_col4, #T_70c3f_row2_col0, #T_70c3f_row2_col1, #T_70c3f_row2_col2, #T_70c3f_row2_col3, #T_70c3f_row2_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_70c3f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_70c3f_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_70c3f_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_70c3f_level0_col2\" class=\"col_heading level0 col2\" >rationale</th>\n",
       "      <th id=\"T_70c3f_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_70c3f_level0_col4\" class=\"col_heading level0 col4\" >embedding_sim_oracle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_70c3f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_70c3f_row0_col0\" class=\"data row0 col0\" >Describe recurrent neural networks ?</td>\n",
       "      <td id=\"T_70c3f_row0_col1\" class=\"data row0 col1\" >Recurrent Neural Networks (RNNs) are specialized neural networks designed for sequential data processing. They maintain a hidden state that captures information from past inputs, allowing...</td>\n",
       "      <td id=\"T_70c3f_row0_col2\" class=\"data row0 col2\" >Here is the optimized instruction: Question: Describe recurrent neural networks? Context: [1] «Recurrent neural network | A recurrent neural network (RNN) is a class of...</td>\n",
       "      <td id=\"T_70c3f_row0_col3\" class=\"data row0 col3\" >Here is the optimized response: Question: Describe recurrent neural networks? Context: [1] «Recurrent neural network | A recurrent neural network (RNN) is a class of...</td>\n",
       "      <td id=\"T_70c3f_row0_col4\" class=\"data row0 col4\" >✔️ [0.7107194662094116]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70c3f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_70c3f_row1_col0\" class=\"data row1 col0\" >What is the subspaced similarity between different r in LoRA ?</td>\n",
       "      <td id=\"T_70c3f_row1_col1\" class=\"data row1 col1\" >In LoRA (Linearly-organized Recurrent Attention), the `subspaced similarity between different r` refers to a measure of similarity between different attention contexts (represented by `r`) within...</td>\n",
       "      <td id=\"T_70c3f_row1_col2\" class=\"data row1 col2\" >Question: What is the subspaced similarity between different r in LoRA ? Context: [1] «Black lory | The black lory (\"Chalcopsitta atra\") also known as...</td>\n",
       "      <td id=\"T_70c3f_row1_col3\" class=\"data row1 col3\" >Here is the rewritten response in the specified format: **Question:** What is the subspaced similarity between different r in LoRA ? **Context:** [1] «Black lory...</td>\n",
       "      <td id=\"T_70c3f_row1_col4\" class=\"data row1 col4\" >✔️ [0.7598505616188049]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70c3f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_70c3f_row2_col0\" class=\"data row2 col0\" >Describe scaled dot product attention</td>\n",
       "      <td id=\"T_70c3f_row2_col1\" class=\"data row2 col1\" >Scaled dot-product attention is a pivotal component of transformer models, facilitating effective capture of dependencies across input sequences. It operates by computing the dot product...</td>\n",
       "      <td id=\"T_70c3f_row2_col2\" class=\"data row2 col2\" >Question: Describe scaled dot product attention Context: [1] «Dot product | In mathematics, the dot product or scalar product is an algebraic operation that takes...</td>\n",
       "      <td id=\"T_70c3f_row2_col3\" class=\"data row2 col3\" >Here is the optimized instruction: **Question:** Describe scaled dot product attention **Context:** [1] «Dot product | In mathematics, the dot product or scalar product is...</td>\n",
       "      <td id=\"T_70c3f_row2_col4\" class=\"data row2 col4\" >✔️ [0.6725741624832153]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fba4e96f430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.682947099208832 / 3  (89.4): 100%|██████████| 3/3 [00:03<00:00,  1.33s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1f342 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1f342 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1f342_row0_col0, #T_1f342_row0_col1, #T_1f342_row0_col2, #T_1f342_row0_col3, #T_1f342_row0_col4, #T_1f342_row1_col0, #T_1f342_row1_col1, #T_1f342_row1_col2, #T_1f342_row1_col3, #T_1f342_row1_col4, #T_1f342_row2_col0, #T_1f342_row2_col1, #T_1f342_row2_col2, #T_1f342_row2_col3, #T_1f342_row2_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1f342\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1f342_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_1f342_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_1f342_level0_col2\" class=\"col_heading level0 col2\" >rationale</th>\n",
       "      <th id=\"T_1f342_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_1f342_level0_col4\" class=\"col_heading level0 col4\" >embedding_sim_oracle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1f342_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1f342_row0_col0\" class=\"data row0 col0\" >Describe recurrent neural networks ?</td>\n",
       "      <td id=\"T_1f342_row0_col1\" class=\"data row0 col1\" >Recurrent Neural Networks (RNNs) are specialized neural networks designed for sequential data processing. They maintain a hidden state that captures information from past inputs, allowing...</td>\n",
       "      <td id=\"T_1f342_row0_col2\" class=\"data row0 col2\" >Answer: A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle, allowing it to exhibit dynamic...</td>\n",
       "      <td id=\"T_1f342_row0_col3\" class=\"data row0 col3\" >Recurrent neural networks.</td>\n",
       "      <td id=\"T_1f342_row0_col4\" class=\"data row0 col4\" >✔️ [0.7559572458267212]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f342_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1f342_row1_col0\" class=\"data row1 col0\" >What is the subspaced similarity between different r in LoRA ?</td>\n",
       "      <td id=\"T_1f342_row1_col1\" class=\"data row1 col1\" >In LoRA (Linearly-organized Recurrent Attention), the `subspaced similarity between different r` refers to a measure of similarity between different attention contexts (represented by `r`) within...</td>\n",
       "      <td id=\"T_1f342_row1_col2\" class=\"data row1 col2\" >Question: What is the subspaced similarity between different r in LoRA ?\n",
       "\n",
       "Context: ...</td>\n",
       "      <td id=\"T_1f342_row1_col3\" class=\"data row1 col3\" >I don't know</td>\n",
       "      <td id=\"T_1f342_row1_col4\" class=\"data row1 col4\" >✔️ [0.9791585803031921]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f342_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1f342_row2_col0\" class=\"data row2 col0\" >Describe scaled dot product attention</td>\n",
       "      <td id=\"T_1f342_row2_col1\" class=\"data row2 col1\" >Scaled dot-product attention is a pivotal component of transformer models, facilitating effective capture of dependencies across input sequences. It operates by computing the dot product...</td>\n",
       "      <td id=\"T_1f342_row2_col2\" class=\"data row2 col2\" >Question: Describe scaled dot product attention\n",
       "\n",
       "Context: ...</td>\n",
       "      <td id=\"T_1f342_row2_col3\" class=\"data row2 col3\" >I don't know</td>\n",
       "      <td id=\"T_1f342_row2_col4\" class=\"data row2 col4\" >✔️ [0.9478312730789185]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fba4ea46a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = dict(num_threads=1, display_progress=True, display_table=3)\n",
    "compiled_oracle = teleprompter_oracle.compile(OracleRAFT(),trainset=trainset, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_oracle.save(\"./ragcompiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, llamas are related to humans but not directly. Llamas belong to the Camelidae family, which also includes camels, alpacas, guanacos, and vicuñas. These animals share a common ancestor with early primates that lived millions of years ago. However, this relationship is distant when compared to other animals we are more closely related to, like apes or monkeys.\n",
      "\n",
      "In terms of evolutionary biology, humans and llamas diverged from the same lineage millions of years ago. So in a broad sense, there's an ancestral connection between them through their common forebears. However, it is essential to note that this relationship doesn't imply any significant genetic similarity or close relatedness between the two species today.\n"
     ]
    }
   ],
   "source": [
    "results = collection.generate.near_text(\n",
    "    query=\"What animals are llamas related to?\",\n",
    "    limit=5,\n",
    "    grouped_task=\"Answer the question: Are llamas related to humans ?\"\n",
    ")\n",
    "print(results.generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weavy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
