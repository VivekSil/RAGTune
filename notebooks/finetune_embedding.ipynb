{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551753b7-6cd2-4f81-aec0-da119e4705ad",
   "metadata": {
    "id": "551753b7-6cd2-4f81-aec0-da119e4705ad"
   },
   "source": [
    "# Finetune Embeddings\n",
    "\n",
    "In this notebook, we show users how to finetune their own embedding models.\n",
    "\n",
    "We go through three main sections:\n",
    "1. Preparing the data (our `generate_qa_embedding_pairs` function makes this easy)\n",
    "2. Finetuning the model (using our `SentenceTransformersFinetuneEngine`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afd542-fc47-44ac-aed0-b3684108dba5",
   "metadata": {
    "id": "99afd542-fc47-44ac-aed0-b3684108dba5"
   },
   "source": [
    "## Generate Corpus\n",
    "\n",
    "First, we create the corpus of text chunks by leveraging LlamaIndex to load some financial PDFs, and parsing/chunking into plain text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e973679e",
   "metadata": {
    "id": "e973679e"
   },
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-openai\n",
    "# %pip install llama-index-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb",
   "metadata": {
    "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c42620",
   "metadata": {
    "id": "73c42620"
   },
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18",
   "metadata": {
    "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = \"./data/\"\n",
    "TRAIN_FILES = [folder_path + i for i in os.listdir(folder_path)][:-1]\n",
    "VAL_FILES = [[folder_path + i for i in os.listdir(folder_path)][-1]]\n",
    "\n",
    "TRAIN_CORPUS_FPATH = \"./data/train_corpus.json\"\n",
    "VAL_CORPUS_FPATH = \"./data/val_corpus.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da871c1-9d58-467a-92fd-06ed3d94534b",
   "metadata": {
    "id": "1da871c1-9d58-467a-92fd-06ed3d94534b"
   },
   "outputs": [],
   "source": [
    "def load_corpus(files, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Loading files {files}\")\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=files)\n",
    "    docs = reader.load_data()\n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    parser = SentenceSplitter()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Parsed {len(nodes)} nodes\")\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53056d8b-3b4c-4364-9b07-a375aa84330b",
   "metadata": {
    "id": "53056d8b-3b4c-4364-9b07-a375aa84330b"
   },
   "source": [
    "We do a very naive train/val split by having the Lyft corpus as the train dataset, and the Uber corpus as the val dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3651c77-d085-4fbc-bb34-61f143ad6674",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "554a6636780246c8a19d1efe7a6e4786",
      "6748733283a34725ba6365f3c1fb1c1d"
     ]
    },
    "id": "d3651c77-d085-4fbc-bb34-61f143ad6674",
    "outputId": "6cba47f2-022b-44e2-b523-33d7f5c2a2f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files ['./data/2305.14314v1.pdf', './data/2405.20202v1.pdf', './data/1706.03762v5.pdf', './data/2106.09685v2.pdf']\n",
      "Loaded 78 docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5129906a3e47369160f8916a214124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 106 nodes\n",
      "Loading files ['./data/2405.15731v1.pdf']\n",
      "Loaded 22 docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e486b9e01c490989b6c4cc752b8fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 39 nodes\n"
     ]
    }
   ],
   "source": [
    "train_nodes = load_corpus(TRAIN_FILES, verbose=True)\n",
    "val_nodes = load_corpus(VAL_FILES, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4482c48-844b-448b-9552-3f38b455645c",
   "metadata": {
    "id": "b4482c48-844b-448b-9552-3f38b455645c"
   },
   "source": [
    "### Generate synthetic queries\n",
    "\n",
    "Now, we use an LLM (llama3-8b-8192) to generate questions using each text chunk in the corpus as context.\n",
    "\n",
    "Each pair of (generated question, text chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f",
   "metadata": {
    "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f"
   },
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666001e2",
   "metadata": {
    "id": "666001e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef43fe59-a29c-481b-b086-e98e55016d3e",
   "metadata": {
    "id": "ef43fe59-a29c-481b-b086-e98e55016d3e",
    "outputId": "52b0b7a7-08af-4b26-8812-8601759e9895"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [04:20<00:00,  2.46s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [01:52<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "\n",
    "train_dataset = generate_qa_embedding_pairs(\n",
    "    llm=Groq(model=\"llama3-8b-8192\",api_key=os.environ[\"GROQ_API_KEY\"]), nodes=train_nodes\n",
    ")\n",
    "val_dataset = generate_qa_embedding_pairs(\n",
    "    llm=Groq(model=\"llama3-8b-8192\",api_key=os.environ[\"GROQ_API_KEY\"]), nodes=val_nodes\n",
    ")\n",
    "\n",
    "train_dataset.save_json(\"train_dataset.json\")\n",
    "val_dataset.save_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd093246-fa7d-4eed-b27a-f4cb452b93a0",
   "metadata": {
    "id": "743f163c-25df-4c18-9abe-05052b034d70"
   },
   "outputs": [],
   "source": [
    "### [Optional] Load\n",
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62368cb8-a303-48b1-8429-5e3655abcc3b",
   "metadata": {
    "id": "62368cb8-a303-48b1-8429-5e3655abcc3b"
   },
   "source": [
    "## Run Embedding Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0",
   "metadata": {
    "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0"
   },
   "outputs": [],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6efae9c64cdc4a92a248cf1619349958",
      "82a58350abe74a59b39686504e56ddb7",
      "c938a8515da340fa8567502eb4ab1379",
      "cec7526cf6d74ab5a90b5a2adecb8dcf",
      "e8177356c92541939bdc0d7f51a88dd2",
      "1155eb2d55b446639814729da89f2a8f",
      "8db118818c7e4dacb623944d8888e0a2",
      "2c7498da6664460ab67ac5fc72fcd565",
      "dfa0168357b74c7f900e49c4cb38b4eb",
      "39c5ef016f2f4d73bd9d78e081b46f47",
      "2d40419c72754123b95d7a4f3430cac3",
      "aba92340280a4601a19f4a8707c45fba",
      "e03c93e272574b46a7bb8ca5e389b354"
     ]
    },
    "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc",
    "outputId": "1ed4e13d-9a58-4510-8cfa-b648f8e2f990"
   },
   "outputs": [],
   "source": [
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_output_path=\"minilm-finetuned\",\n",
    "    val_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "78dab7a09dd640619d80e986baf37249",
      "b025b81ebe21403498679bf916626ff9",
      "e737aae9a5f4459c97df630e63b9c487"
     ]
    },
    "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b",
    "outputId": "e0fcf135-31b4-44f1-dad3-eea0d00297b8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f22e5084f64183a4342ff3a0f63914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6251ebd1f44bbbbde26c33d3a9f3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa964cb0a5b4c1c929258ca0b6402c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493",
   "metadata": {
    "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493"
   },
   "outputs": [],
   "source": [
    "embed_model = finetune_engine.get_finetuned_model()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "llamaindex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
